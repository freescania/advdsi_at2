{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threaded-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Data Processing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surprised-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in data\n",
    "\n",
    "beer_dat = pd.read_csv(r\"C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\data\\raw\\beer_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1586614 entries, 0 to 1586613\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   brewery_id          1586614 non-null  int64  \n",
      " 1   brewery_name        1586599 non-null  object \n",
      " 2   review_time         1586614 non-null  int64  \n",
      " 3   review_overall      1586614 non-null  float64\n",
      " 4   review_aroma        1586614 non-null  float64\n",
      " 5   review_appearance   1586614 non-null  float64\n",
      " 6   review_profilename  1586266 non-null  object \n",
      " 7   beer_style          1586614 non-null  object \n",
      " 8   review_palate       1586614 non-null  float64\n",
      " 9   review_taste        1586614 non-null  float64\n",
      " 10  beer_name           1586614 non-null  object \n",
      " 11  beer_abv            1518829 non-null  float64\n",
      " 12  beer_beerid         1586614 non-null  int64  \n",
      "dtypes: float64(6), int64(3), object(4)\n",
      "memory usage: 157.4+ MB\n"
     ]
    }
   ],
   "source": [
    "beer_dat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scenic-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide up data\n",
    "num_cols = ['review_aroma','review_appearance','review_palate','review_taste']\n",
    "\n",
    "cat_cols = ['brewery_id']\n",
    "y_cat_cols = ['beer_style']\n",
    "key_feat = ['brewery_id','review_aroma','review_appearance','review_palate','review_taste','beer_style']\n",
    "\n",
    "df_cleaned = beer_dat[key_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smaller-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-1a50bd7a9b68>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[num_cols] = sc.fit_transform(df_cleaned[num_cols])\n",
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py:4305: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py:4305: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "#Instantiate SC\n",
    "sc = StandardScaler()\n",
    "#Scale numerics\n",
    "df_cleaned[num_cols] = sc.fit_transform(df_cleaned[num_cols])\n",
    "\n",
    "\n",
    "#scale Cat\n",
    "X_cat = df_cleaned[cat_cols].astype('category')\n",
    "df_cleaned.drop(cat_cols, axis=1, inplace=True)\n",
    "ode = OrdinalEncoder()\n",
    "X_cat_y = pd.DataFrame(ode.fit_transform(df_cleaned[y_cat_cols]))\n",
    "X_cat_y.columns = y_cat_cols\n",
    "X_cat_y = X_cat_y.astype(int)\n",
    "df_cleaned.drop(y_cat_cols, axis=1, inplace=True)\n",
    "X_cat_cols = pd.concat([X_cat, X_cat_y ], axis=1)\n",
    "\n",
    "#recombine as X\n",
    "X = pd.concat([df_cleaned, X_cat_cols ], axis=1)\n",
    "\n",
    "X = X.drop(columns=['brewery_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorporate-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1586614 entries, 0 to 1586613\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count    Dtype  \n",
      "---  ------             --------------    -----  \n",
      " 0   review_aroma       1586614 non-null  float64\n",
      " 1   review_appearance  1586614 non-null  float64\n",
      " 2   review_palate      1586614 non-null  float64\n",
      " 3   review_taste       1586614 non-null  float64\n",
      " 4   beer_style         1586614 non-null  int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 54.5 MB\n"
     ]
    }
   ],
   "source": [
    "X_cat_y.beer_style.unique()\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "corresponding-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\src\\data\")\n",
    "from sets import subset_x_y, split_sets_by_time, save_sets, split_sets_random\n",
    "os.chdir(r\"C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exterior-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def pop_target(df, target_col, to_numpy=False):\n",
    "    \"\"\"Extract target variable from dataframe and convert to nympy arrays if required\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe\n",
    "    target_col : str\n",
    "        Name of the target variable\n",
    "    to_numpy : bool\n",
    "        Flag stating to convert to numpy array or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame/Numpy array\n",
    "        Subsetted Pandas dataframe containing all features\n",
    "    pd.DataFrame/Numpy array\n",
    "        Subsetted Pandas dataframe containing the target\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    target = df_copy.pop(target_col)\n",
    "    \n",
    "    if to_numpy:\n",
    "        df_copy = df_copy.to_numpy()\n",
    "        target = target.to_numpy()\n",
    "    \n",
    "    return df_copy, target\n",
    "\n",
    "# Solution\n",
    "def split_sets_random(df, target_col, test_ratio=0.2, to_numpy=False):\n",
    "    \"\"\"Split sets randomly\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    target_col : str\n",
    "        Name of the target column\n",
    "    test_ratio : float\n",
    "        Ratio used for the validation and testing sets (default: 0.2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy Array\n",
    "        Features for the training set\n",
    "    Numpy Array\n",
    "        Target for the training set\n",
    "    Numpy Array\n",
    "        Features for the validation set\n",
    "    Numpy Array\n",
    "        Target for the validation set\n",
    "    Numpy Array\n",
    "        Features for the testing set\n",
    "    Numpy Array\n",
    "        Target for the testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    features, target = pop_target(df=df, target_col=target_col, to_numpy=to_numpy)\n",
    "    \n",
    "    X_data, X_test, y_data, y_test = train_test_split(features, target, test_size=test_ratio, random_state=8)\n",
    "    \n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=val_ratio, random_state=8)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "noble-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train test split, change to numpy\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_sets_random(X, target_col='beer_style', test_ratio=0.2, to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "coordinate-brazil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951968,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train = np.array([y_train])\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spread-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sets(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, path='../data/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acknowledged-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\src\\models\")\n",
    "from pytorch import PytorchDataset\n",
    "os.chdir(r\"C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virtual-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "downtown-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Pytorch sets\n",
    "train_dataset = PytorchDataset(X=X_train, y=y_train)\n",
    "val_dataset = PytorchDataset(X=X_val, y=y_val)\n",
    "test_dataset = PytorchDataset(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "presidential-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\src\\models\")\n",
    "from null import NullModel\n",
    "from performance import print_class_perf\n",
    "from MClasspytorch import PytorchMultiClass, train_classification, test_classification\n",
    "os.chdir(r\"C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "corresponding-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 0.0742157299405022\n",
      "F1 Training: 0.01025487603110527\n"
     ]
    }
   ],
   "source": [
    "# fit a baseline model for reference\n",
    "\n",
    "baseline_model = NullModel(target_type='classification')\n",
    "y_base = baseline_model.fit_predict(y_train)\n",
    "\n",
    "#results\n",
    "print_class_perf(y_base, y_train, set_name='Training', average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "sudden-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(train_data, model, criterion, optimizer, batch_size, device, scheduler=None, generate_batch=None):\n",
    "    \"\"\"Train a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    optimizer: torch.optim\n",
    "        Optimizer\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    scheduler : torch.optim.lr_scheduler\n",
    "        Pytorch Scheduler used for updating learning rate\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(feature)\n",
    "        \n",
    "        # Calculate loss for given batch\n",
    "        loss = criterion(output, target_class.long())\n",
    "\n",
    "        # Calculate global loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate global accuracy\n",
    "        train_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_loss / len(train_data), train_acc / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bizarre-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(test_data, model, criterion, batch_size, device, generate_batch=None):\n",
    "    \"\"\"Calculate performance of a Pytorch multi-class classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : torch.utils.data.Dataset\n",
    "        Pytorch dataset\n",
    "    model: torch.nn.Module\n",
    "        Pytorch Model\n",
    "    criterion: function\n",
    "        Loss function\n",
    "    bacth_size : int\n",
    "        Number of observations per batch\n",
    "    device : str\n",
    "        Name of the device used for the model\n",
    "    collate_fn : function\n",
    "        Function defining required pre-processing steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Loss score\n",
    "    Float:\n",
    "        Accuracy Score\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    # Create data loader\n",
    "    data = DataLoader(test_data, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    \n",
    "    # Iterate through data by batch of observations\n",
    "    for feature, target_class in data:\n",
    "        \n",
    "        # Load data to specified device\n",
    "        feature, target_class = feature.to(device), target_class.to(device)\n",
    "        \n",
    "        # Set no update to gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Make predictions\n",
    "            output = model(feature)\n",
    "            \n",
    "            # Calculate loss for given batch\n",
    "            loss = criterion(output, target_class.long())\n",
    "\n",
    "            # Calculate global loss\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate global accuracy\n",
    "            test_acc += (output.argmax(1) == target_class).sum().item()\n",
    "\n",
    "    return test_loss / len(test_data), test_acc / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "sublime-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchMultiClass(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(PytorchMultiClass, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 32)\n",
    "        self.layer_out = nn.Linear(32, 104) #from 4, 104 number of classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.layer_1(x)), training=self.training)\n",
    "        x = self.layer_out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ordinary-petroleum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchMultiClass(\n",
      "  (layer_1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (layer_out): Linear(in_features=32, out_features=104, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Train Multiclass\n",
    "model = PytorchMultiClass(X_train.shape[1])\n",
    "\n",
    "#Set device no GPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "isolated-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "\n",
    "# Set optimizer and Learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "signed-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "residential-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\angus\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.5%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 1\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 2\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 3\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 4\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 5\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 6\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 7\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 8\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 9\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 10\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 11\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 12\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 13\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 14\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 15\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 16\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 17\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 18\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 19\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 20\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 21\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 22\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 23\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 24\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 25\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 26\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 27\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 28\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 29\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 30\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 31\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 32\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 33\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 34\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 35\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 36\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 37\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 38\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 39\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 40\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 41\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 42\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 43\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 44\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 45\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 46\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 47\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 48\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "Epoch: 49\n",
      "\t(train)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n",
      "\t(valid)\t|\tLoss: 0.1433\t|\tAcc: 7.4%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset, model=model, criterion=criterion, optimizer=optimizer, batch_size=BATCH_SIZE, device=device)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "limiting-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "torch.save(model.state_dict(), r'C:\\Users\\Angus\\Documents\\UTS MDSI\\Advanced DSI\\Projects\\advdsi_at2\\models\\NN_4feat.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-sharing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
